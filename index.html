<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mutual Information-Driven Self-supervised Point Cloud Pre-training</title>
  <link rel="icon" type="image/x-icon" href="static/images/GPICTURE_icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mutual Information-Driven Self-supervised Point Cloud Pre-training</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Weichen Xu, Tianhao Fu, Jian Cao, Xinxin Xu, Xixin Cao, Xing Zhang</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Peking University, Beijing 100871, China<br>ACM Multimedia 2024, Knowledge-Based Systems submitted</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/RookieXwc/GPICTURE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/overview_480p_3×.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered"> -->
      <h2 style="text-align: left; font-size: 20px;">
        <b>We conduct a careful study seeking answers to several questions:</b>
          <br>(1) What are the issues with self-supervised point cloud pre-training represented by GeoMAE?
          <br>(2) How can high-level features be systematically utilized in 3D point clouds similar to 2D images?
          <br>(3) Why can high-level features serve as pretext tasks to benefit downstream tasks?
          <!-- <br>(1) What is the promotion of high-level features in 2D images?
          <br>(2) Are low-level features in 3D point clouds sufficient as pretext tasks? -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning the universal representation from unlabeled 3D point clouds is essential for the autonomous driving community 
            that aims to improve generalization and safety. Generative self-supervised point cloud pre-training with low-level 
            features as pretext tasks is a mainstream paradigm. However, from the perspective of mutual information, it remains 
            trapped within spatial information and entangled representations. Here, we propose a generalized generative self-supervised 
            point cloud pre-training framework GPICTURE. Firstly, high-level features represented by Seal features are demonstrated to 
            exhibit higher mutual information with the input point cloud. We utilize the high-level voxel features as an additional pretext 
            task to enhance the understanding of semantic information. Next, inter-class and intra-class discrimination-guided masking (I2Mask) 
            is utilized to set the masking ratio for each superclass adaptively. Furthermore, CKA-guided hierarchical reconstruction and 
            differential-gated progressive learning are used to weight the low-level and high-level reconstruction losses spatially and 
            temporally. On Waymo and nuScenes, we achieve 75.55% mAP and 73.13% mAPH for 3D object detection, 79.7% mIoU for 3D semantic 
            segmentation, and 18.8% mIoU for occupancy prediction. Complete theoretical analyses and extensive experiments demonstrate that 
            high-level features with strong semantic information can enhance the mutual information between latent features and high-level 
            features, and the input point cloud.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-4">Current Issues in Low-level Features</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 style="text-align: left;">
            From the perspective of mutual information, we identify <b>two issues</b> with the current paradigm:
            <br>(1) The pre-training process guided by low-level features focuses on learning the discrimination of spatial information while neglecting semantic information;
            <br>(2) The low information content in pretext tasks leads to entangled representations.
            <br>To address these issues, we propose <b>two solutions</b>:
            <br>(1) The mutual information between latent features and semantic features needs to be enhanced.
            <br>(2) the mutual information between latent features and the input point cloud needs to be enhanced.
          </h2>
          <!-- <img src="static/images/figure_1.jpg" alt="MY ALT TEXT"/> -->
          <!-- <img src="static/images/figure_1.jpg" alt="MY ALT TEXT" style="width: 50%; height: auto;"/> -->
          <div class="image-grid">
            <div class="image-container">
                <img src="static/images/figure_1_a.jpg" alt="Image 1" style="width: 75%; height: auto;"/>
                <p>(a)</p>
            </div>
            <div class="image-container">
                <img src="static/images/figure_1_b.jpg" alt="Image 2" style="width: 75%; height: auto;"/>
                <p>(b)</p>
            </div>
            <div class="image-container">
                <img src="static/images/figure_1_c.jpg" alt="Image 3" style="width: 75%; height: auto;"/>
                <p>(c)</p>
            </div>
            <div class="image-container">
                <img src="static/images/figure_1_d.jpg" alt="Image 4" style="width: 75%; height: auto;"/>
                <p>(d)</p>
            </div>
        </div>
          <h2 style="text-align: center;">
              <br><b>Figure 1: </b>The CKA between the encoder from SSL method
              GeoMAE and the encoder from SL for (a) 3D object
              detection and (b) 3D semantic segmentation for the same
              input; The CKA between two frames from the (c) same scene
              and (d) different scenes when using the same encoder.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-4">Proposed Method</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/figure_2.jpg" alt="MY ALT TEXT" style="width: 100%; height: auto;"/>
          <h2 style="text-align: justify;">
              <br><b>GPICTURE</b>: (a) The self-supervised learning process in 3D point clouds. (b) Architecture overview of the 
              generalized generative self-supervised point cloud pre-training framework GPICTURE. The raw 3D point clouds are fed 
              into (c) the high-level voxel feature generation module to obtain high-level voxel features. (d) Inter-class and intra-class 
              discrimination-guided masking strategy is applied based on the attributes of high-level voxel features. (e) CKA-guided 
              hierarchical reconstruction and (f) differential-gated progressive learning are used to weight the low-level and high-level 
              reconstruction losses spatially and temporally. 
          </h2>
          <h2 style="text-align: justify;">
            <b>Reconstruction Target.</b> The Seal feature heatmaps for example point cloud scene in nuScenes are shown 
            in Fig. 3(a). At the same time, the ground truth of the same scene in three downstream tasks are visualized in 
            Fig. 3(b). It can be seen  Seal feature and the real scene have a high semantic consistency. On the one hand, 
            the Seal feature is strong for road-related objects. On the other hand, the feature decreases with distance from ego. Based
             on this observation, we consider reconstructing Seal features as an additional pretext task. Specifically, all raw point 
             clouds that have not been voxelized are fed into the MinkUNet (Res16UNet34C) pre-trained by Seal. Each point cloud aggregates
              spatial and feature information from other point clouds. We obtain the point cloud Seal feature. Subsequently, we voxelize 
              in the same way as inference. The average pooling is employed to aggregate sparse features that are located in the same voxel.
               This aggregation is based on the unique sparse coordinates. Finally, we obtain the Seal features of all non-empty voxels.
          <br><b>Inter-class and Intra-class Discrimination-guided Masking.</b> Firstly, 8 superclasses that can represent autonomous driving
           scenarios are obtained in an unsupervised manner. On the one hand, for inter-class, we propose Fastest Class Sampling (FCS), 
           which divides 8 superclasses into 3 groups based on discriminative cues and sets base mask ratios. On the other hand, we define
            intra-class consistency coefficient for each superclass and modulate the base mask ratios, which helps to adjust the difficulty 
            level of the reconstruction task.
          <br><b>CKA-guided Hierarchical Reconstruction.</b> We compute the layer-wise CKA between the encoder from the self-supervised learning 
          method GeoMAE and the encoder from supervised learning for 3D object detection, 3D semantic segmentation, and 
          occupancy prediction. Then, we define the layer-wise mean of the CKA for the three downstream tasks as the proportion of low-level information.
          Accordingly, we assign the weights for low-level and high-level reconstruction. This ensures that point cloud encoders follow hierarchical feature learning.
          <br><b>Differential-gated Progressive Learning.</b> We use the measure of mutual information, CKA, between epochs t-2 and t-1 to calculate the activation differential.
          Then, we calculate the average CKA of the first four layers as the completion degree of low-level feature reconstruction (CLFR) at the current epoch t.
          High-level feature reconstruction will only be initiated when the CLFR exceeds the gating factor, which will enhance the stability of the training process.
          </h2>
          <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/> -->
          <div class="image-grid">
            <div class="image-container">
                <img src="static/images/figure_3.png" alt="Image 1" style="width: 75%; height: auto;"/>
                <p><b>Figure 3</b>: (a) Heatmap for ground truth Seal feature and encoder output pretrained w/o and w/ Seal feature.
                   (b) Ground truth visualization for detection, segmentation, and occupancy prediction.</p>
            </div>
            <div class="image-container">
                <img src="static/images/algo_1.png" alt="Image 2" style="width: 75%; height: auto;"/>
                <p><b>Algorithm 1</b>: Fastest Class Sampling</p>
            </div>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Visualization</h2>
      <div id="results-carousel" class="carousel results-carousel">
      <div class="image-container">
          <!-- Your image here -->
          <!-- <img src="static/images/figure_d1.png" alt="MY ALT TEXT"/> -->
          <img src="static/images/figure_d1.png" alt="Image 1" style="width: 30%; height: auto;"/>
          <h2 class="subtitle has-text-centered">
            <b>Figure 3.Appendix</b>: Heatmaps for more scenes. Each row represents a scene.
          </h2>
      </div>
       <div class="image-container">
        <!-- Your image here -->
        <!-- <img src="static/images/figure_4.png" alt="MY ALT TEXT"/> -->
        <img src="static/images/figure_4.png" alt="Image 1" style="width: 60%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Figure 4</b>: (a) Ground truth visualization of occupancy prediction. (b) The mask ratio distribution derived by I2Mask.
        </h2>
      </div>
      <div class="image-container">
        <!-- Your image here -->
        <!-- <img src="static/images/figure_d1.png" alt="MY ALT TEXT"/> -->
        <img src="static/images/figure_5.png" alt="Image 1" style="width: 30%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Figure 5</b>: The disparity in the weight distribution of each attention layer in the encoder. (a) and (b) represent mean and variance, respectively.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Experimental Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="image-container">
        <img src="static/images/table_3.png" alt="Image 1" style="width: 75%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 3</b>: Comparisons of 3D object detection between GPICTURE and other self-supervised learning methods on Waymo validation set.
          † indicates the results are from the original paper. ∗ presents re-implemented by OpenPCDet. ‘Epochs’ and ‘Fraction’ denote the pre-training
          epochs and dataset fraction used for pre-training. The improvement compared to training from scratch is indicated with red superscripts.
        </h2>
      </div>
      <div class="image-container">
        <img src="static/images/table_5.png" alt="Image 1" style="width: 55%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 5</b>: Comparisons of 3D semantic segmentation between GPICTURE and other self-supervised methods on nuScenes val set.
        </h2>
        <img src="static/images/table_6.png" alt="Image 1" style="width: 40%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 6</b>: Comparisons of occupancy prediction between GPICTURE and other self-supervised methods on OpenOccupancy val set.
        </h2>
      </div>
      <div class="image-container">
        <img src="static/images/table_8.png" alt="Image 1" style="width: 30%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 8</b>: Impacts of the base mask ratio for superclass partition in inter-class discrimination-guided masking.
        </h2>
        <img src="static/images/table_9.png" alt="Image 1" style="width: 30%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 9</b>: Impacts of expected number of superclass partition in inter-class discrimination-guided masking.
        </h2>
      </div>
      <div class="image-container">
        <img src="static/images/table_12.png" alt="Image 1" style="width: 75%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 12</b>: Ablation study of pre-training, reconstruction target, mask sampling strategy, CHR, and DGPL on the Waymo val set.
        </h2>
      </div>
      <div class="image-container">
        <img src="static/images/table_13.png" alt="Image 1" style="width: 75%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 13</b>: Comparisons of different high-level features for pretext tasks in 3D object detection on Waymo val set.
        </h2>
      </div>
      <div class="image-container">
        <img src="static/images/table_17.png" alt="Image 1" style="width: 40%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 17</b>: Comparison with no pre-training on different data scale in 3D object detection using L2 mAPH on Waymo val set.
        </h2>
        <img src="static/images/table_19.png" alt="Image 1" style="width: 45%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <b>Table 19</b>: Time cost of self-supervised learning methods during pre-training on 8 A100-SXM4-40GB GPUs on Waymo.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
